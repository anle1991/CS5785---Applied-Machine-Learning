{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import *\n",
    "import time\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "# nltk.download() if not installed nltk\n",
    "from nltk.corpus import stopwords  # Stopwords: ‘the’, ‘is’, ‘are’...\n",
    "from nltk.stem.porter import * # Stem: gamer, gaming, game -> game\n",
    "from nltk.tokenize import RegexpTokenizer # Regexp: set rule to just tokenize word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feat, test_feat = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for line in open(\"./data/features_train/features_resnet1000_train.csv\"):\n",
    "    l = line.strip().split(\",\")\n",
    "    train_feat.append(l)\n",
    "train_feat = np.array(train_feat)\n",
    "for line in open(\"./data/features_test/features_resnet1000_test.csv\"):\n",
    "    l = line.strip().split(\",\")\n",
    "    test_feat.append(l)\n",
    "test_feat = np.array(test_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dict(path):\n",
    "    dictionary = {}\n",
    "    \n",
    "    ## glob.glob return paths matching given pattern\n",
    "    for i in range(10000):\n",
    "        file_name = str(i) + \".txt\"\n",
    "        file = os.path.join(path, file_name)\n",
    "        with open(file, \"r\") as file_content:\n",
    "            content = file_content.read()\n",
    "            content = np.char.lower(content) # Lowercase\n",
    "            content = re.sub('[^\\w\\s]', ' ', str(content)) # Define split\n",
    "            \n",
    "            for word in tokenizer.tokenize(content): # Remove punctuation\n",
    "                try:\n",
    "                    if word not in stop_words: # Remove stopwords\n",
    "                        w = stemmer.stem(word) # Stem\n",
    "                        if w in dictionary:\n",
    "                            dictionary[w] += 1\n",
    "                        else:\n",
    "                            dictionary[w] = 1\n",
    "                except:\n",
    "                    pass \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "global_desc_dict = create_dict(\"./data/descriptions_train/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag Of Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_desc_bow(file_path, global_dict, threshold, file_count):\n",
    "    dict_thresh = {}   \n",
    "    i = 0\n",
    "    for w in global_dict:\n",
    "        if global_dict[w] >= threshold:\n",
    "            dict_thresh[w] = i\n",
    "            i = i + 1\n",
    "    \n",
    "    dict_freq = {}\n",
    "    for i in range(file_count):\n",
    "        file_name = str(i) + \".txt\"\n",
    "        file = os.path.join(file_path, file_name)\n",
    "        with open(file, \"r\") as content_file:\n",
    "            content = content_file.read()\n",
    "            content = np.char.lower(content)\n",
    "            content = re.sub('[^\\w\\s]', ' ', str(content))\n",
    "            stemmer = PorterStemmer()\n",
    "            \n",
    "            cur = [0] * len(dict_thresh)\n",
    "            for word in content.split():\n",
    "                try:\n",
    "                    if word not in stopwords.words(\"english\"):\n",
    "                        w = stemmer.stem(word)\n",
    "                        if w in dict_thresh:\n",
    "                            cur[dict_thresh[w]] += 1\n",
    "                except:\n",
    "                    pass\n",
    "            dict_freq[file_name.split('/')[-1]] = cur\n",
    "    return dict_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_desc_dict_freq = create_desc_bow(\"./data/descriptions_train/\", global_desc_dict, 20, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_desc_dict_freq = create_desc_bow(\"./data/descriptions_test/\", global_desc_dict, 20, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## KNN with description to feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit Knn with description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for k in train_desc_dict_freq:\n",
    "    X.append(train_desc_dict_freq[k])\n",
    "    y.append(k)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "knn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat_dict = {}\n",
    "for i in range(len(train_feat)):\n",
    "    name = int(train_feat[i][0].split(\"/\")[1].split(\".\")[0])\n",
    "    train_feat_dict[name] = np.array(train_feat[i][1:], dtype=float)\n",
    "\n",
    "def norm_1(feat1, feat2):\n",
    "    feat1 = np.array(feat1, dtype=float)\n",
    "    feat2 = np.array(feat2, dtype=float)\n",
    "    return np.linalg.norm(feat1 - feat2)\n",
    "\n",
    "def candidate_images(target_feat, feats):\n",
    "    feat_score = {}\n",
    "    for f in feats:\n",
    "        name = f[0,].split(\"/\")[1]\n",
    "        score = norm_1(target_feat, f[1:])\n",
    "        feat_score[name] = score\n",
    "    sorted_feat_score = sorted(feat_score.items(), key=operator.itemgetter(1))\n",
    "    res = []\n",
    "    for i in range(20):\n",
    "        res.append(sorted_feat_score[i][0])\n",
    "    res = np.array(res)\n",
    "    return res;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(filename):\n",
    "    d = np.array(test_desc_dict_freq[filename])\n",
    "    pred = knn.predict(d.reshape(1, -1))\n",
    "    pred_value = int(pred[0].split(\".\")[0])\n",
    "    agg = train_feat_dict[pred_value]\n",
    "    candidates = candidate_images(agg, test_feat)\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamp():\n",
    "    millis = int(round(time.time() * 1000))\n",
    "    return millis\n",
    "\n",
    "def create_submission_csv():\n",
    "    submission = []\n",
    "    submission.append([\"Descritpion_ID\", \"Top_20_Image_IDs\"])\n",
    "    count = 0\n",
    "    for d in test_desc_dict_freq:\n",
    "        line = []\n",
    "        line.append(d)\n",
    "        candidates = \" \".join(predict(d))\n",
    "        line.append(candidates)\n",
    "        submission.append(line)\n",
    "        count += 1\n",
    "    filename = 'submission' + '_' + str(get_timestamp()) + '.csv'\n",
    "    with open(filename, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
